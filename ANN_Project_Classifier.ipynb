{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANN_Project_Classifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8lti72d4fxq"
      },
      "source": [
        "# Train Custom Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giwV9dJggysd",
        "outputId": "cbae0f21-6934-4d86-ded7-8f32d962f9da"
      },
      "source": [
        "from google.colab import drive, files\n",
        "drive.mount('/content/drive/')\n",
        "path = \"/content/drive/MyDrive/ANN-Project/\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqUb0PjGwjdJ"
      },
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torch.nn as nn  # All neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim  # For all Optimization algorithms, SGD, Adam, etc.\n",
        "import pandas as pd\n",
        "from torch.utils.data import (\n",
        "    Dataset,\n",
        "    DataLoader,\n",
        ")  # Gives easier dataset managment and creates mini batches"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XKGq_7JvB3C",
        "outputId": "c27108cb-89e2-4605-ed94-51c8ca14b501"
      },
      "source": [
        "imported_weights = torch.load(path+'layer_3_output_weights_tensor.pth')\n",
        "imported_bias = torch.load(path+'layer_3_output_bias_tensor.pth')\n",
        "print(imported_weights)\n",
        "print(imported_bias)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.0147, -0.0082,  0.0009,  ..., -0.0163,  0.0048, -0.0019],\n",
            "        [ 0.0096, -0.0054,  0.0079,  ..., -0.0095, -0.0122, -0.0027],\n",
            "        [-0.0088, -0.0098, -0.0210,  ..., -0.0142, -0.0027, -0.0104],\n",
            "        ...,\n",
            "        [ 0.0377, -0.0108,  0.0385,  ..., -0.0146,  0.0211, -0.0067],\n",
            "        [ 0.0171, -0.0198,  0.0186,  ..., -0.0051,  0.0050, -0.0094],\n",
            "        [ 0.0110, -0.0172,  0.0157,  ...,  0.0113,  0.0073, -0.0129]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0077, -0.0071, -0.0068,  ...,  0.0499,  0.0134,  0.0499],\n",
            "       requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxn0Y5A4RtyU"
      },
      "source": [
        "#df = pd.read_csv(path+'dataset.csv')\n",
        "#df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RX4gHnKlkc8n",
        "outputId": "2a6ff2eb-0dd2-4a12-b9bc-5f93a0019930"
      },
      "source": [
        "class Tekken3PoseDataset(Dataset):\n",
        "    def __init__(self, csv_file):\n",
        "        self.annotations = pd.read_csv(csv_file)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        pose_data = self.annotations.iloc[index, 2:]\n",
        "        pose_data.fillna(0, inplace=True)\n",
        "        pose_data = torch.tensor(pose_data).float()\n",
        "        y_label = torch.tensor(int(self.annotations.iloc[index, 1]))\n",
        "\n",
        "        return (pose_data, y_label)\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 1e-3\n",
        "batch_size = 32\n",
        "num_epochs = 10\n",
        "\n",
        "# Load Data\n",
        "dataset = Tekken3PoseDataset(\n",
        "    csv_file=path+\"dataset.csv\"\n",
        ")\n",
        "\n",
        "# 70:30 -> 4285\n",
        "\n",
        "train_set, test_set = torch.utils.data.random_split(dataset, [3000, 1285])\n",
        "train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(Net, self).__init__()\n",
        "\n",
        "      # First hidden layer (fc)\n",
        "      self.h1 = nn.Linear(225, 4096)\n",
        "      # Second hidden layer (fc)\n",
        "      self.h2 = nn.Linear(4096, 4096)\n",
        "      #  hidden layer that outputs our 2 labels\n",
        "      self.output = nn.Linear(4096, 2)\n",
        "#      self.dropout = nn.Dropout(p=0.4)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = nn.functional.relu(self.h1(x))\n",
        "      x = nn.functional.relu(self.h2(x))\n",
        "#      x = self.dropout(nn.functional.relu(self.h2(x)))\n",
        "      x = nn.functional.softmax(self.output(x))\n",
        "      return x\n",
        "\n",
        "model = Net()\n",
        "\n",
        "model.h2.weight.data = imported_weights\n",
        "model.h2.bias.data = imported_bias\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train Network\n",
        "for epoch in range(num_epochs):\n",
        "    losses = []\n",
        "\n",
        "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "        # Get data to cuda if possible\n",
        "        data = data.to(device=device)\n",
        "        targets = targets.to(device=device)\n",
        "\n",
        "        # forward\n",
        "        scores = model(data)\n",
        "        loss = criterion(scores, targets)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # gradient descent or adam step\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Cost at epoch {epoch} is {sum(losses)/len(losses)}\")\n",
        "\n",
        "# Check accuracy on training to see how good our model is\n",
        "def check_accuracy(loader, model):\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device=device)\n",
        "            y = y.to(device=device)\n",
        "\n",
        "            scores = model(x)\n",
        "            _, predictions = scores.max(1)\n",
        "            num_correct += (predictions == y).sum()\n",
        "            num_samples += predictions.size(0)\n",
        "\n",
        "        print(\n",
        "            f\"Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}\"\n",
        "        )\n",
        "\n",
        "    model.train()\n",
        "\n",
        "\n",
        "print(\"Checking accuracy on Training Set\")\n",
        "check_accuracy(train_loader, model)\n",
        "\n",
        "print(\"Checking accuracy on Test Set\")\n",
        "check_accuracy(test_loader, model)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Cost at epoch 0 is 0.8272242653877178\n",
            "Cost at epoch 1 is 0.8271134432325972\n",
            "Cost at epoch 2 is 0.827445886870648\n",
            "Cost at epoch 3 is 0.827113447037149\n",
            "Cost at epoch 4 is 0.8275567077575846\n",
            "Cost at epoch 5 is 0.8270026394661437\n",
            "Cost at epoch 6 is 0.8273350786655507\n",
            "Cost at epoch 7 is 0.8271134425985053\n",
            "Cost at epoch 8 is 0.8270026274183964\n",
            "Cost at epoch 9 is 0.8270026312229481\n",
            "Checking accuracy on Training Set\n",
            "Got 1458 / 3000 with accuracy 48.60\n",
            "Checking accuracy on Test Set\n",
            "Got 635 / 1285 with accuracy 49.42\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkhjTWjahjHK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}