{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANN_Project_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UV56A74b37gB"
      },
      "source": [
        "# Train VGG16 on Custom Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giwV9dJggysd",
        "outputId": "0bea7e4e-0a3e-47ef-f7fa-d16945294fb8"
      },
      "source": [
        "from google.colab import drive, files\n",
        "drive.mount('/content/drive/')\n",
        "path = \"/content/drive/MyDrive/ANN-Project/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RX4gHnKlkc8n",
        "outputId": "b434677a-1e9b-4742-9511-a60317e2fa10"
      },
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torch.nn as nn  # All neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim  # For all Optimization algorithms, SGD, Adam, etc.\n",
        "import torchvision.transforms as transforms  # Transformations we can perform on our dataset\n",
        "import torchvision.models as models\n",
        "import os\n",
        "import pandas as pd\n",
        "from skimage import io\n",
        "from torch.utils.data import (\n",
        "    Dataset,\n",
        "    DataLoader,\n",
        ")  # Gives easier dataset managment and creates mini batches\n",
        "from PIL import Image\n",
        "\n",
        "class Tekken3Dataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, transform=None):\n",
        "        self.annotations = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = os.path.join(self.root_dir, self.annotations.iloc[index, 0])\n",
        "        image = io.imread(img_path)\n",
        "        PIL_image = Image.fromarray (image)\n",
        "        y_label = torch.tensor(int(self.annotations.iloc[index, 1]))\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(PIL_image)\n",
        "\n",
        "        return (image, y_label)\n",
        "\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 1e-3\n",
        "batch_size = 32\n",
        "num_epochs = 10\n",
        "\n",
        "# Specify transforms using torchvision.transforms as transforms library\n",
        "transformations = transforms.Compose([\n",
        "    transforms.Resize(255),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load Data\n",
        "dataset = Tekken3Dataset(\n",
        "    csv_file=path+\"dataset_only_images.csv\",\n",
        "    root_dir=path+\"dataset\",\n",
        "    transform=transformations,\n",
        ")\n",
        "\n",
        "# 70:30 -> 7808\n",
        "\n",
        "train_set, test_set = torch.utils.data.random_split(dataset, [5465, 2343])\n",
        "train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Model\n",
        "\n",
        "model = models.vgg16()\n",
        "model.to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train Network\n",
        "for epoch in range(num_epochs):\n",
        "    losses = []\n",
        "\n",
        "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "        # Get data to cuda if possible\n",
        "        data = data.to(device=device)\n",
        "        targets = targets.to(device=device)\n",
        "\n",
        "        # forward\n",
        "        scores = model(data)\n",
        "        loss = criterion(scores, targets)\n",
        "\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # gradient descent or adam step\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Cost at epoch {epoch} is {sum(losses)/len(losses)}\")\n",
        "\n",
        "# Check accuracy on training to see how good our model is\n",
        "def check_accuracy(loader, model):\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device=device)\n",
        "            y = y.to(device=device)\n",
        "\n",
        "            scores = model(x)\n",
        "            _, predictions = scores.max(1)\n",
        "            num_correct += (predictions == y).sum()\n",
        "            num_samples += predictions.size(0)\n",
        "\n",
        "        print(\n",
        "            f\"Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}\"\n",
        "        )\n",
        "\n",
        "    model.train()\n",
        "\n",
        "\n",
        "print(\"Checking accuracy on Training Set\")\n",
        "check_accuracy(train_loader, model)\n",
        "\n",
        "print(\"Checking accuracy on Test Set\")\n",
        "check_accuracy(test_loader, model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Cost at epoch 0 is 1.6913418173789978\n",
            "Cost at epoch 1 is 0.7017916963811506\n",
            "Cost at epoch 2 is 0.6994726696209601\n",
            "Cost at epoch 3 is 0.6969407943257114\n",
            "Cost at epoch 4 is 0.6965081405918501\n",
            "Cost at epoch 5 is 0.6962810294669971\n",
            "Cost at epoch 6 is 0.6965277198462458\n",
            "Cost at epoch 7 is 0.694534480920312\n",
            "Cost at epoch 8 is 0.6961131566449216\n",
            "Cost at epoch 9 is 0.6957225611335353\n",
            "Checking accuracy on Training Set\n",
            "Got 2745 / 5465 with accuracy 50.23\n",
            "Checking accuracy on Test Set\n",
            "Got 1160 / 2343 with accuracy 49.51\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0qRMfUf4CNF"
      },
      "source": [
        "Extra Epoches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KS0ojzid0ZYF",
        "outputId": "7ab6f3e4-8221-43e5-8dde-b30dc3941569"
      },
      "source": [
        "# Train Network\n",
        "for epoch in range(num_epochs):\n",
        "    losses = []\n",
        "\n",
        "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "        # Get data to cuda if possible\n",
        "        data = data.to(device=device)\n",
        "        targets = targets.to(device=device)\n",
        "\n",
        "        # forward\n",
        "        scores = model(data)\n",
        "        loss = criterion(scores, targets)\n",
        "\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # gradient descent or adam step\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Cost at epoch {epoch} is {sum(losses)/len(losses)}\")\n",
        "\n",
        "print(\"Checking accuracy on Training Set\")\n",
        "check_accuracy(train_loader, model)\n",
        "\n",
        "print(\"Checking accuracy on Test Set\")\n",
        "check_accuracy(test_loader, model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost at epoch 0 is 0.6953630823838083\n",
            "Cost at epoch 1 is 0.6961498131528933\n",
            "Cost at epoch 2 is 0.6946044417849758\n",
            "Cost at epoch 3 is 0.69536895082708\n",
            "Cost at epoch 4 is 0.6943775234166641\n",
            "Cost at epoch 5 is 0.6939951366151286\n",
            "Cost at epoch 6 is 0.6940497574750443\n",
            "Cost at epoch 7 is 0.6957850968628599\n",
            "Cost at epoch 8 is 0.6947413341343751\n",
            "Cost at epoch 9 is 0.6943481114872715\n",
            "Checking accuracy on Training Set\n",
            "Got 2720 / 5465 with accuracy 49.77\n",
            "Checking accuracy on Test Set\n",
            "Got 1183 / 2343 with accuracy 50.49\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5Cpbc3h4FCh"
      },
      "source": [
        "# Export Model Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5ixD4r7wMn9"
      },
      "source": [
        "torch.save(model.state_dict(), path+'model_weights.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4HdcjNu-vbq",
        "outputId": "eaab67fb-1c16-4985-85fe-07e07a5ea85a"
      },
      "source": [
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): ReLU(inplace=True)\n",
            "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): ReLU(inplace=True)\n",
            "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pn6p0_fI4L6D"
      },
      "source": [
        "# Desired Deep Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBw4-QggFvWn"
      },
      "source": [
        "temp = model.classifier[3].weight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfG6hjqHGAaB",
        "outputId": "c247b529-b38d-4ff4-c531-d90143cc62c0"
      },
      "source": [
        "export_data = temp.cpu().detach().numpy()\n",
        "export_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.01472528, -0.0082471 ,  0.00089048, ..., -0.01633802,\n",
              "         0.00478626, -0.00193277],\n",
              "       [ 0.00957653, -0.00540264,  0.00794996, ..., -0.0094647 ,\n",
              "        -0.01219382, -0.00268113],\n",
              "       [-0.00880944, -0.00980182, -0.02101649, ..., -0.01422585,\n",
              "        -0.00270699, -0.01039774],\n",
              "       ...,\n",
              "       [ 0.03772741, -0.01078148,  0.03850559, ..., -0.01457085,\n",
              "         0.02113776, -0.00673066],\n",
              "       [ 0.01711877, -0.01976011,  0.01861127, ..., -0.00514485,\n",
              "         0.00497056, -0.0093764 ],\n",
              "       [ 0.01096603, -0.0172389 ,  0.01574892, ...,  0.01127687,\n",
              "         0.0073191 , -0.01291022]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCKPgP3RH4D4"
      },
      "source": [
        "torch.save(model.classifier[3].weight, path+'layer_3_output_weights_tensor.pth')\n",
        "torch.save(model.classifier[3].bias, path+'layer_3_output_bias_tensor.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tawki0z9IuJm"
      },
      "source": [
        "torch.save(export_data, path+'layer_3_output_weights_numpy.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "My1x_X8eJN0e"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}